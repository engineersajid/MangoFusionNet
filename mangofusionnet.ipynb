{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13180057,"sourceType":"datasetVersion","datasetId":8352290}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Mango Classification\nThis notebook demonstrates how to build an ensemble model using EfficientNetB0, VGG16, and ResNet50 for mango classification","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0, VGG16, ResNet50\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as eff_pre\nfrom tensorflow.keras.applications.vgg16 import preprocess_input as vgg_pre\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as res_pre\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, BatchNormalization, Concatenate, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport random\nimport math\nimport cv2","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:48:37.595807Z","iopub.execute_input":"2025-10-01T04:48:37.596257Z","iopub.status.idle":"2025-10-01T04:48:53.194547Z","shell.execute_reply.started":"2025-10-01T04:48:37.596236Z","shell.execute_reply":"2025-10-01T04:48:53.193972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Check GPU Availability\nVerify if GPU is available for faster training","metadata":{}},{"cell_type":"code","source":"# Check GPU availability\nprint(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n\n# Setup distribution strategy (only if GPUs are available)\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    strategy = tf.distribute.MirroredStrategy()\n    print(\" Strategy initialized for GPU\")\n    print(\"Number of replicas:\", strategy.num_replicas_in_sync)\nelse:\n    strategy = tf.distribute.get_strategy()\n    print(\" Using default strategy for CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:49:05.244486Z","iopub.execute_input":"2025-10-01T04:49:05.244975Z","iopub.status.idle":"2025-10-01T04:49:06.175316Z","shell.execute_reply.started":"2025-10-01T04:49:05.244930Z","shell.execute_reply":"2025-10-01T04:49:06.174510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Data Loading and Preparation\nLoad and preprocess the mango dataset","metadata":{}},{"cell_type":"code","source":"# Dataset configuration\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.AUTOTUNE\n\n# Dataset paths\nbase_path = '/kaggle/input/mango7200-12/MangoMerged'\ntrain_dir = os.path.join(base_path, 'train')\nval_dir = os.path.join(base_path, 'val')\ntest_dir = os.path.join(base_path, 'test')\n\n# Get class names\nclass_names = sorted([\n    d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))\n])\nprint(\"All Name of Classes:\", class_names)\nNUM_CLASSES = len(class_names)\n\n# Load datasets\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    train_dir,\n    label_mode='categorical',\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n).prefetch(AUTOTUNE)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    val_dir,\n    label_mode='categorical',\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n).prefetch(AUTOTUNE)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    test_dir,\n    label_mode='categorical',\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n).prefetch(AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:49:12.975492Z","iopub.execute_input":"2025-10-01T04:49:12.976301Z","iopub.status.idle":"2025-10-01T04:49:21.781988Z","shell.execute_reply.started":"2025-10-01T04:49:12.976273Z","shell.execute_reply":"2025-10-01T04:49:21.781275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4.Visualize few Images from a TensorFlow Dataset**","metadata":{}},{"cell_type":"code","source":"\ndef visualize_images_to_pdf(dataset, class_names, num_images=15, pdf_path=\"visualized_images.pdf\"):\n    with PdfPages(pdf_path) as pdf:\n        plt.figure(figsize=(15, 10))\n\n        # Take just one batch\n        for images, labels in dataset.take(1):\n            for i in range(min(num_images, images.shape[0])):\n                ax = plt.subplot(3, 5, i + 1)\n\n                # Convert image for plotting\n                img = images[i].numpy().astype(\"uint8\")\n                plt.imshow(img)\n\n                # Convert one-hot label to index\n                label_index = tf.argmax(labels[i]).numpy()\n                plt.title(class_names[label_index])\n\n                plt.axis(\"off\")\n\n        plt.tight_layout()\n        pdf.savefig()\n        plt.show()\n        plt.close()\n\n    print(f\"ðŸ“„ Visualization saved as {pdf_path}\")\n\nvisualize_images_to_pdf(train_ds, class_names, num_images=15)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:49:27.980505Z","iopub.execute_input":"2025-10-01T04:49:27.981243Z","iopub.status.idle":"2025-10-01T04:49:32.683264Z","shell.execute_reply.started":"2025-10-01T04:49:27.981214Z","shell.execute_reply":"2025-10-01T04:49:32.682468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Build the hybrid Model\nCreate an ensemble of EfficientNetB0, MobileNetV2, and ResNet50","metadata":{}},{"cell_type":"code","source":"input_shape = (224, 224, 3)\ninputs = Input(shape=input_shape)\n\n# Preprocess for each model separately\neff_input = Lambda(eff_pre, name=\"eff_pre\")(inputs)\nvgg_input = Lambda(vgg_pre, name=\"vgg_pre\")(inputs)\nres_input = Lambda(res_pre, name=\"res_pre\")(inputs)\n\n# Instantiate base models\neffnet_base = EfficientNetB0(weights=\"imagenet\", include_top=False)\nvgg16_base = VGG16(weights=\"imagenet\", include_top=False)\nresnet_base = ResNet50(weights=\"imagenet\", include_top=False)\n\n# Freeze base models\nfor base_model in [effnet_base, vgg16_base, resnet_base]:\n    base_model.trainable = False\n\n# Extract features\neff_features = effnet_base(eff_input)\nvgg_features = vgg16_base(vgg_input)\nres_features = resnet_base(res_input)\n\nfeat1 = GlobalAveragePooling2D()(eff_features)\nfeat2 = GlobalAveragePooling2D()(vgg_features)\nfeat3 = GlobalAveragePooling2D()(res_features)\n\n# Concatenate features\nmerged = Concatenate()([feat1, feat2, feat3])\n\n# Classifier head\nx = Dense(512, activation=\"relu\")(merged)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\n\nx = Dense(256, activation=\"relu\")(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\noutputs = Dense(12, activation=\"softmax\")(x)\n\n# Build model\nmodel = Model(inputs, outputs)\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:49:39.984126Z","iopub.execute_input":"2025-10-01T04:49:39.984409Z","iopub.status.idle":"2025-10-01T04:49:50.161157Z","shell.execute_reply.started":"2025-10-01T04:49:39.984386Z","shell.execute_reply":"2025-10-01T04:49:50.160579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Train the Model\nTrain the model with callbacks and early stops","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Callbacks\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=10,        \n    restore_best_weights=True,\n    verbose=1\n)\n\ncheckpoint = ModelCheckpoint(\n    \"best_model.h5\",\n    monitor=\"val_loss\",\n    save_best_only=True,\n    mode=\"min\",\n    verbose=1\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=3,\n    min_lr=1e-6,\n    verbose=1\n)\n\n# Compile\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",   # use 'sparse_categorical_crossentropy' if labels are integer\n    metrics=[\"accuracy\"]\n)\n\n# Train\nhistory = model.fit(\n    train_ds,              #training dataset\n    validation_data=val_ds, #validation datase\n    epochs=30,\n    batch_size=32,\n    callbacks=[early_stop, checkpoint, reduce_lr]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:50:00.251077Z","iopub.execute_input":"2025-10-01T04:50:00.251663Z","iopub.status.idle":"2025-10-01T04:56:30.568120Z","shell.execute_reply.started":"2025-10-01T04:50:00.251640Z","shell.execute_reply":"2025-10-01T04:56:30.567167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Evaluate the Model\nEvaluate model performance on test data","metadata":{}},{"cell_type":"code","source":"# Evaluate model\ntest_loss, test_acc = model.evaluate(test_ds)\nprint(f\"\\nTest Accuracy (Keras evaluate): {test_acc:.4f}\")\n\ny_true = np.concatenate([y for x, y in test_ds], axis=0)\ny_pred = model.predict(test_ds)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_true, axis=1)\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n\n# Compute confusion matrix\ncm = confusion_matrix(y_true_classes, y_pred_classes)\n\n# Save confusion matrices in multiple colors\ncolormaps = {\n    \"blue\": \"Blues\",\n    \"red\": \"Reds\",\n    \"green\": \"Greens\",\n    \"magenta\": \"magma\",\n    \"parula\": \"viridis\" \n}\n\nfor color_name, cmap_name in colormaps.items():\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        cm, annot=True, fmt=\"d\", cmap=cmap_name,\n        xticklabels=class_names, yticklabels=class_names\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion Matrix ({color_name.capitalize()})\")\n    plt.tight_layout()\n    \n    filename = f\"{color_name}_confusion_matrix.pdf\"\n    plt.savefig(filename)\n\n    #  Only display Green in notebook\n    if color_name == \"green\":\n        plt.show()\n    else:\n        plt.close()\n    \n    print(f\"Saved {filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:56:43.975730Z","iopub.execute_input":"2025-10-01T04:56:43.976026Z","iopub.status.idle":"2025-10-01T05:00:18.395301Z","shell.execute_reply.started":"2025-10-01T04:56:43.975989Z","shell.execute_reply":"2025-10-01T05:00:18.394477Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Plot Training History\nVisualize training and validation performance","metadata":{}},{"cell_type":"code","source":"def plot_training_history(history, model_name=\"Model\", save_as_pdf=True):\n    \"\"\"Plot training history\"\"\"\n    epochs = len(history.history['accuracy'])\n    epoch_list = list(range(1, epochs + 1))\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    fig.suptitle(f'{model_name} Performance', fontsize=16)\n    fig.subplots_adjust(top=0.88, wspace=0.3)\n    \n    # Accuracy plot\n    ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy', marker='o')\n    ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n    ax1.grid(True, linestyle='--', alpha=0.7)\n    ax1.set_xticks(np.arange(1, epochs + 1, max(1, epochs // 10)))\n    ax1.set_ylabel('Accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.set_title('Accuracy')\n    ax1.legend(loc=\"best\")\n    \n    # Loss plot\n    ax2.plot(epoch_list, history.history['loss'], label='Train Loss', marker='o')\n    ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss', marker='o')\n    ax2.grid(True, linestyle='--', alpha=0.7)\n    ax2.set_xticks(np.arange(1, epochs + 1, max(1, epochs // 10)))\n    ax2.set_ylabel('Loss')\n    ax2.set_xlabel('Epoch')\n    ax2.set_title('Loss')\n    ax2.legend(loc=\"best\")\n    \n    plt.tight_layout()\n    if save_as_pdf:\n        fig.savefig(f\"{model_name}_training_history.pdf\", dpi=300)\n    plt.show()\n\n# Generate two separate PDFs\nplot_training_history(history, \"MangoFusionNet\", save_as_pdf=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T05:00:26.690198Z","iopub.execute_input":"2025-10-01T05:00:26.690488Z","iopub.status.idle":"2025-10-01T05:00:27.170754Z","shell.execute_reply.started":"2025-10-01T05:00:26.690466Z","shell.execute_reply":"2025-10-01T05:00:27.169997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  9.predict class and confidence","metadata":{}},{"cell_type":"code","source":"# Function to predict class and confidence\ndef predict(model, image):\n    img_array = np.expand_dims(image, axis=0)\n    preds = model.predict(img_array, verbose=0)[0]\n    pred_idx = np.argmax(preds)\n    confidence = round(100 * np.max(preds), 2)\n    return pred_idx, confidence\n\n# Function to visualize random predictions and save to PDF\ndef visualize_predictions_to_pdf(model, dataset, class_names, pdf_path=\"predictions.pdf\", num_images=9):\n    # Convert dataset to list of (images, labels)\n    all_images, all_labels = [], []\n    for images, labels in dataset:\n        all_images.append(images.numpy())\n        all_labels.append(labels.numpy())\n    all_images = np.concatenate(all_images)\n    all_labels = np.concatenate(all_labels)\n\n    # Choose random indices\n    indices = random.sample(range(len(all_images)), num_images)\n\n    with PdfPages(pdf_path) as pdf:\n        plt.figure(figsize=(15, 15))\n\n        for i, idx in enumerate(indices):\n            ax = plt.subplot(3, 3, i + 1)\n            image = all_images[idx]\n            label = all_labels[idx]\n\n            plt.imshow(image.astype(\"uint8\"))\n\n            # Predict\n            pred_idx, confidence = predict(model, image)\n            predicted_class = class_names[pred_idx]\n\n            # Handle one-hot vs integer labels\n            if len(label.shape) > 0 and label.shape[0] > 1:  \n                actual_idx = np.argmax(label)\n            else:\n                actual_idx = int(label)\n            \n            actual_class = class_names[actual_idx]\n\n            plt.title(f\"Actual: {actual_class}\\nPredicted: {predicted_class}\\nConf: {confidence}%\", fontsize=10)\n            plt.axis(\"off\")\n\n        plt.tight_layout()\n        pdf.savefig(dpi=300)\n        plt.show()\n        plt.close()\n\n    print(f\"ðŸ“„ Random predictions saved to {pdf_path}\")\n\nvisualize_predictions_to_pdf(model, test_ds, class_names, pdf_path=\"hybrid_predictions.pdf\", num_images=9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T05:00:31.589809Z","iopub.execute_input":"2025-10-01T05:00:31.590511Z","iopub.status.idle":"2025-10-01T05:01:51.435633Z","shell.execute_reply.started":"2025-10-01T05:00:31.590487Z","shell.execute_reply":"2025-10-01T05:01:51.435028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **10.Grad-CAM Implementation (XAI)**","metadata":{}},{"cell_type":"code","source":"# Image preprocessing per model\n\ndef get_img_array(img_path, size, model_type=\"efficientnet\"):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    array = np.expand_dims(array, axis=0)\n    if model_type==\"efficientnet\":\n        array = eff_pre(array)   \n    elif model_type==\"vgg16\":\n        array = vgg_pre(array)   \n    elif model_type==\"resnet50\":\n        array = res_pre(array)   \n    return array\n\n# Grad-CAM heatmap generator\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        inputs=model.input,\n        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        class_output = predictions[:, pred_index]\n    grads = tape.gradient(class_output, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\n# Display & save Grad-CAM\n\ndef display_and_save_gradcam(img_path, heatmaps, titles, pdf, alpha=0.4):\n    original = cv2.imread(img_path)\n    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n    original = cv2.resize(original, (224, 224))\n    n = len(heatmaps)\n    fig, axes = plt.subplots(n, 2, figsize=(10, 5 * n))  # n rows, 2 cols\n    fig.suptitle(os.path.basename(img_path), fontsize=16, y=1.02)\n    for i, (heatmap, title) in enumerate(zip(heatmaps, titles)):\n        # Original\n        axes[i, 0].imshow(original)\n        axes[i, 0].set_title(\"Original\")\n        axes[i, 0].axis('off')\n        # Heatmap overlay\n        heatmap_resized = cv2.resize(heatmap, (224, 224))\n        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n        superimposed_img = cv2.addWeighted(original, 1 - alpha, heatmap_colored, alpha, 0)\n        axes[i, 1].imshow(superimposed_img)\n        axes[i, 1].set_title(title)\n        axes[i, 1].axis('off')\n    plt.tight_layout(rect=[0, 0, 1, 0.97])\n    pdf.savefig(fig)\n    plt.close(fig)\n\n\n# Build Grad-CAM model wrapper\n\ndef build_gradcam_model(base_model, last_conv_layer_name, num_classes):\n    x = base_model.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n    return model, last_conv_layer_name\n\n# Configuration\n\nIMG_SIZE = (224, 224)\nNUM_CLASSES = 12\nN_PER_CLASS = 1  # random images per class\nbase_path = '/kaggle/input/mango7200-12/MangoMerged'\n\n# Load and wrap models\n\nefficientnet_base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=IMG_SIZE+(3,))\nvgg_base = VGG16(weights='imagenet', include_top=False, input_shape=IMG_SIZE+(3,))\nresnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=IMG_SIZE+(3,))\n\nfor base in [efficientnet_base, vgg_base, resnet_base]:\n    base.trainable = False\n\neff_model, eff_layer = build_gradcam_model(efficientnet_base, \"block7a_project_bn\", NUM_CLASSES)\nvgg_model, vgg_layer = build_gradcam_model(vgg_base, \"block5_conv3\", NUM_CLASSES)\nres_model, res_layer = build_gradcam_model(resnet_base, \"conv5_block3_out\", NUM_CLASSES)\n\n# Collect images per class\n\nclass_to_images = {}\nfor root, _, files in os.walk(base_path):\n    for file in files:\n        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n            class_name = os.path.basename(root)\n            if class_name not in class_to_images:\n                class_to_images[class_name] = []\n            class_to_images[class_name].append(os.path.join(root, file))\n\nselected_images = {\n    cls: random.sample(imgs, min(N_PER_CLASS, len(imgs)))\n    for cls, imgs in class_to_images.items()\n}\n\npdf_output = \"gradcam_per_class_1_random.pdf\"\nwith PdfPages(pdf_output) as pdf:\n\n    # One page per image\n    for cls, img_paths in selected_images.items():\n        print(f\"Processing class '{cls}' - single-page-per-image\")\n        for img_path in img_paths:\n            eff_array = get_img_array(img_path, IMG_SIZE, \"efficientnet\")\n            vgg_array = get_img_array(img_path, IMG_SIZE, \"vgg16\")\n            res_array = get_img_array(img_path, IMG_SIZE, \"resnet50\")\n            eff_heatmap = make_gradcam_heatmap(eff_array, eff_model, eff_layer)\n            vgg_heatmap = make_gradcam_heatmap(vgg_array, vgg_model, vgg_layer)\n            res_heatmap = make_gradcam_heatmap(res_array, res_model, res_layer)\n            display_and_save_gradcam(img_path, [eff_heatmap, vgg_heatmap, res_heatmap],\n                                     [\"EfficientNetB0\",\"VGG16\",\"ResNet50\"], pdf)\n\n    # One page per class\n    for cls, img_paths in selected_images.items():\n        print(f\"Processing class '{cls}' - single-page-per-class\")\n        n_imgs = len(img_paths)\n        fig, axes = plt.subplots(n_imgs, 4, figsize=(20, 5 * n_imgs))  # 4 cols: Original + 3 models\n        if n_imgs == 1:\n            axes = np.expand_dims(axes, axis=0)  # make 2D array shape (1,4)\n        fig.suptitle(f\"{cls} - {N_PER_CLASS} Random Images\", fontsize=16, y=1.02)\n        for row, img_path in enumerate(img_paths):\n            eff_array = get_img_array(img_path, IMG_SIZE, \"efficientnet\")\n            vgg_array = get_img_array(img_path, IMG_SIZE, \"vgg16\")\n            res_array = get_img_array(img_path, IMG_SIZE, \"resnet50\")\n            eff_heatmap = make_gradcam_heatmap(eff_array, eff_model, eff_layer)\n            vgg_heatmap = make_gradcam_heatmap(vgg_array, vgg_model, vgg_layer)\n            res_heatmap = make_gradcam_heatmap(res_array, res_model, res_layer)\n\n            original = cv2.imread(img_path)\n            original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n            original = cv2.resize(original, (224, 224))\n\n            axes[row][0].imshow(original)\n            axes[row][0].set_title(\"Original\")\n            axes[row][0].axis(\"off\")\n\n            for col, (heatmap, title) in enumerate(zip([eff_heatmap, vgg_heatmap, res_heatmap],\n                                                       [\"EfficientNetB0\",\"VGG16\",\"ResNet50\"]), start=1):\n                heatmap_resized = cv2.resize(heatmap, (224,224))\n                heatmap_colored = cv2.applyColorMap(np.uint8(255*heatmap_resized), cv2.COLORMAP_JET)\n                heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n                superimposed_img = cv2.addWeighted(original, 0.6, heatmap_colored, 0.4, 0)\n                axes[row][col].imshow(superimposed_img)\n                axes[row][col].set_title(title)\n                axes[row][col].axis(\"off\")\n\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        pdf.savefig(fig, dpi=300)\n        plt.show()\n        plt.close(fig)\n\nprint(f\"Grad-CAM results saved to {pdf_output}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T05:02:01.683268Z","iopub.execute_input":"2025-10-01T05:02:01.683778Z","iopub.status.idle":"2025-10-01T05:03:12.826617Z","shell.execute_reply.started":"2025-10-01T05:02:01.683756Z","shell.execute_reply":"2025-10-01T05:03:12.825812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11.Save the model ","metadata":{}},{"cell_type":"code","source":"# Save the model in Keras native format (.keras)\nmodel.save('/kaggle/working/MangoModel.keras', overwrite=True)\nmodel.save('/kaggle/working/MangoModel.h5', overwrite=True)\nprint(\"Model saved at /kaggle/working/MangoModel.keras. Download it from the sidebar.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}